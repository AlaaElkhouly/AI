{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State 1:\n",
      "  Action 0:\n",
      "    To State 1: Probability = 0.8\n",
      "    To State 2: Probability = 0.1\n",
      "    To State 4: Probability = 0.1\n",
      "  Action 1:\n",
      "    To State 0: Probability = 0.1\n",
      "    To State 1: Probability = 0.1\n",
      "    To State 4: Probability = 0.8\n",
      "  Action 2:\n",
      "    To State 0: Probability = 0.8\n",
      "    To State 2: Probability = 0.1\n",
      "    To State 4: Probability = 0.1\n",
      "  Action 3:\n",
      "    To State 0: Probability = 0.1\n",
      "    To State 1: Probability = 0.1\n",
      "    To State 2: Probability = 0.8\n",
      "State 3:\n",
      "  Action 0:\n",
      "    To State 0: Probability = 0.8\n",
      "    To State 4: Probability = 0.1\n",
      "    To State 6: Probability = 0.1\n",
      "  Action 1:\n",
      "    To State 0: Probability = 0.1\n",
      "    To State 2: Probability = 0.1\n",
      "    To State 6: Probability = 0.8\n",
      "  Action 2:\n",
      "    To State 2: Probability = 0.8\n",
      "    To State 4: Probability = 0.1\n",
      "    To State 6: Probability = 0.1\n",
      "  Action 3:\n",
      "    To State 0: Probability = 0.1\n",
      "    To State 2: Probability = 0.1\n",
      "    To State 4: Probability = 0.8\n",
      "State 4:\n",
      "  Action 0:\n",
      "    To State 1: Probability = 0.8\n",
      "    To State 5: Probability = 0.1\n",
      "    To State 7: Probability = 0.1\n",
      "  Action 1:\n",
      "    To State 1: Probability = 0.1\n",
      "    To State 3: Probability = 0.1\n",
      "    To State 7: Probability = 0.8\n",
      "  Action 2:\n",
      "    To State 3: Probability = 0.8\n",
      "    To State 5: Probability = 0.1\n",
      "    To State 7: Probability = 0.1\n",
      "  Action 3:\n",
      "    To State 1: Probability = 0.1\n",
      "    To State 3: Probability = 0.1\n",
      "    To State 5: Probability = 0.8\n",
      "State 5:\n",
      "  Action 0:\n",
      "    To State 2: Probability = 0.8\n",
      "    To State 6: Probability = 0.1\n",
      "    To State 8: Probability = 0.1\n",
      "  Action 1:\n",
      "    To State 2: Probability = 0.1\n",
      "    To State 4: Probability = 0.1\n",
      "    To State 8: Probability = 0.8\n",
      "  Action 2:\n",
      "    To State 4: Probability = 0.8\n",
      "    To State 6: Probability = 0.1\n",
      "    To State 8: Probability = 0.1\n",
      "  Action 3:\n",
      "    To State 2: Probability = 0.1\n",
      "    To State 4: Probability = 0.1\n",
      "    To State 6: Probability = 0.8\n",
      "State 6:\n",
      "  Action 0:\n",
      "    To State 3: Probability = 0.8\n",
      "    To State 6: Probability = 0.1\n",
      "    To State 7: Probability = 0.1\n",
      "  Action 1:\n",
      "    To State 3: Probability = 0.1\n",
      "    To State 5: Probability = 0.1\n",
      "    To State 6: Probability = 0.8\n",
      "  Action 2:\n",
      "    To State 5: Probability = 0.8\n",
      "    To State 6: Probability = 0.1\n",
      "    To State 7: Probability = 0.1\n",
      "  Action 3:\n",
      "    To State 3: Probability = 0.1\n",
      "    To State 5: Probability = 0.1\n",
      "    To State 7: Probability = 0.8\n",
      "State 7:\n",
      "  Action 0:\n",
      "    To State 4: Probability = 0.8\n",
      "    To State 7: Probability = 0.1\n",
      "    To State 8: Probability = 0.1\n",
      "  Action 1:\n",
      "    To State 4: Probability = 0.1\n",
      "    To State 6: Probability = 0.1\n",
      "    To State 7: Probability = 0.8\n",
      "  Action 2:\n",
      "    To State 6: Probability = 0.8\n",
      "    To State 7: Probability = 0.1\n",
      "    To State 8: Probability = 0.1\n",
      "  Action 3:\n",
      "    To State 4: Probability = 0.1\n",
      "    To State 6: Probability = 0.1\n",
      "    To State 8: Probability = 0.8\n",
      "State 8:\n",
      "  Action 0:\n",
      "    To State 5: Probability = 0.8\n",
      "    To State 8: Probability = 0.2\n",
      "  Action 1:\n",
      "    To State 5: Probability = 0.1\n",
      "    To State 7: Probability = 0.1\n",
      "    To State 8: Probability = 0.8\n",
      "  Action 2:\n",
      "    To State 7: Probability = 0.8\n",
      "    To State 8: Probability = 0.2\n",
      "  Action 3:\n",
      "    To State 5: Probability = 0.1\n",
      "    To State 7: Probability = 0.1\n",
      "    To State 8: Probability = 0.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def setup_grid_environment(reward_terminal):\n",
    "    \"\"\"\n",
    "    Define the grid environment with rewards and transition probabilities.\n",
    "    \"\"\"\n",
    "    grid_rewards = np.full(9, -1)  # Default penalty for non-terminal states\n",
    "    grid_rewards[0] = reward_terminal  # Terminal reward (upper-left corner)\n",
    "    grid_rewards[2] = 10  # Terminal reward (upper-right corner)\n",
    "\n",
    "    transitions = np.zeros((9, 4, 9))  # Transition matrix (states, actions, outcomes)\n",
    "    \n",
    "    directions = [(-3, 0), (3, 0), (0, -1), (0, 1)]  # Offset for actions (up, down, left, right)\n",
    "\n",
    "    for state in range(9):\n",
    "        if state in [0, 2]:\n",
    "            continue  # Skip terminal states\n",
    "\n",
    "        for action in range(4):\n",
    "            intended = state + directions[action][0] + directions[action][1]\n",
    "            if not (0 <= intended < 9):\n",
    "                intended = state\n",
    "\n",
    "            transitions[state, action, intended] += 0.8  # Main direction\n",
    "\n",
    "            for perp in [(action + 1) % 4, (action + 3) % 4]:\n",
    "                alternative = state + directions[perp][0] + directions[perp][1]\n",
    "                if not (0 <= alternative < 9):\n",
    "                    alternative = state\n",
    "\n",
    "                transitions[state, action, alternative] += 0.1  # Side directions\n",
    "\n",
    "    return grid_rewards, transitions\n",
    "\n",
    "reward_terminal = 10\n",
    "rewards, transitions = setup_grid_environment(reward_terminal)\n",
    "\n",
    "for state in range(9):\n",
    "    if state in [0, 2]:\n",
    "        continue\n",
    "    print(f\"State {state}:\")\n",
    "    for action in range(4):\n",
    "        print(f\"  Action {action}:\")\n",
    "        for outcome in range(9):\n",
    "            prob = transitions[state, action, outcome]\n",
    "            if prob > 0:\n",
    "                print(f\"    To State {outcome}: Probability = {prob}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized Policy:\n",
      "Policy:\n",
      "T → T\n",
      "↑ ↑ ↑\n",
      "↑ ↑ ↑\n",
      "\n",
      "Value Function:\n",
      "Values:\n",
      "10.00 8.78 10.00\n",
      "8.33 7.38 8.29\n",
      "6.89 6.14 6.94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class PolicyEvaluator:\n",
    "    def __init__(self, rewards, transitions, gamma, initial_policy=None):\n",
    "        self.num_states = len(rewards)\n",
    "        self.num_actions = len(transitions[0])\n",
    "        self.rewards = rewards\n",
    "        self.transitions = transitions\n",
    "        self.gamma = gamma\n",
    "        self.values = np.zeros(self.num_states)\n",
    "        self.policy = (\n",
    "            np.random.randint(0, self.num_actions, self.num_states)\n",
    "            if initial_policy is None\n",
    "            else initial_policy\n",
    "        )\n",
    "\n",
    "    def update_values(self):\n",
    "        change = 0\n",
    "        for state in range(self.num_states):\n",
    "            previous = self.values[state]\n",
    "            selected_action = self.policy[state]\n",
    "            self.values[state] = self.rewards[state] + self.gamma * np.sum(\n",
    "                self.transitions[state, selected_action] * self.values\n",
    "            )\n",
    "            change = max(change, abs(previous - self.values[state]))\n",
    "        return change\n",
    "\n",
    "    def evaluate_policy(self, tolerance=1e-3):\n",
    "        for _ in range(100):\n",
    "            if self.update_values() < tolerance:\n",
    "                break\n",
    "\n",
    "    def refine_policy(self):\n",
    "        adjustments = 0\n",
    "        for state in range(self.num_states):\n",
    "            current_action = self.policy[state]\n",
    "            action_values = [\n",
    "                np.sum(self.transitions[state, a] * self.values) for a in range(self.num_actions)\n",
    "            ]\n",
    "            self.policy[state] = np.argmax(action_values)\n",
    "            if self.policy[state] != current_action:\n",
    "                adjustments += 1\n",
    "        return adjustments\n",
    "\n",
    "    def optimize(self, tolerance=1e-3, visualize=True):\n",
    "        iterations = 0\n",
    "        while iterations < 500:\n",
    "            self.evaluate_policy(tolerance)\n",
    "            if self.refine_policy() == 0:\n",
    "                break\n",
    "            iterations += 1\n",
    "\n",
    "        if visualize:\n",
    "            self.display_results()\n",
    "\n",
    "    def display_results(self):\n",
    "        print(\"Optimized Policy:\")\n",
    "        self.display_grid(self.policy, \"Policy\")\n",
    "        print(\"Value Function:\")\n",
    "        self.display_grid(self.values, \"Values\")\n",
    "\n",
    "    def display_grid(self, data, label):\n",
    "        grid = 3\n",
    "        symbols = [\"\\u2191\", \"\\u2193\", \"\\u2190\", \"\\u2192\"]\n",
    "        print(f\"{label}:\")\n",
    "        for i in range(grid):\n",
    "            row = []\n",
    "            for j in range(grid):\n",
    "                idx = i * grid + j\n",
    "                if data is self.policy:\n",
    "                    row.append(\"T\" if idx in [0, 2] else symbols[data[idx]])\n",
    "                else:\n",
    "                    row.append(f\"{data[idx]:.2f}\")\n",
    "            print(\" \".join(row))\n",
    "        print()\n",
    "\n",
    "rewards, transitions = setup_grid_environment(10)\n",
    "policy_eval = PolicyEvaluator(rewards, transitions, gamma=0.99)\n",
    "policy_eval.optimize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Policy:\n",
      "Policy:\n",
      "T ↓ T\n",
      "↑ ← ←\n",
      "↑ ← ↑\n",
      "\n",
      "Value Function:\n",
      "Final Policy:\n",
      "Policy:\n",
      "T → T\n",
      "← → ↑\n",
      "↑ ↑ ↑\n",
      "\n",
      "Value Function:\n",
      "Final Policy:\n",
      "Policy:\n",
      "T → T\n",
      "← → ↑\n",
      "↑ ↑ ↑\n",
      "\n",
      "Value Function:\n",
      "Final Policy:\n",
      "Policy:\n",
      "T → T\n",
      "← → ↑\n",
      "↑ ↑ ↑\n",
      "\n",
      "Value Function:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ValueOptimizer:\n",
    "    def __init__(self, rewards, transitions, gamma, tolerance=1e-3):\n",
    "        self.num_states = len(rewards)\n",
    "        self.num_actions = len(transitions[0])\n",
    "        self.rewards = rewards\n",
    "        self.transitions = transitions\n",
    "        self.gamma = gamma\n",
    "        self.tolerance = tolerance\n",
    "        self.values = np.zeros(self.num_states)\n",
    "        self.policy = np.zeros(self.num_states, dtype=int)\n",
    "\n",
    "    def iterate_values(self):\n",
    "        max_change = 0\n",
    "        for state in range(self.num_states):\n",
    "            old_value = self.values[state]\n",
    "            value_per_action = [\n",
    "                self.rewards[state] + self.gamma * np.sum(self.transitions[state, a] * self.values)\n",
    "                for a in range(self.num_actions)\n",
    "            ]\n",
    "            self.values[state] = max(value_per_action)\n",
    "            max_change = max(max_change, abs(old_value - self.values[state]))\n",
    "        return max_change\n",
    "\n",
    "    def train(self, max_iterations=500):\n",
    "        for _ in range(max_iterations):\n",
    "            if self.iterate_values() < self.tolerance:\n",
    "                break\n",
    "        self.policy = self.derive_policy()\n",
    "        self.display_results()\n",
    "\n",
    "    def derive_policy(self):\n",
    "        policy = np.zeros(self.num_states, dtype=int)\n",
    "        for state in range(self.num_states):\n",
    "            action_rewards = [\n",
    "                self.rewards[state] + self.gamma * np.sum(self.transitions[state, a] * self.values)\n",
    "                for a in range(self.num_actions)\n",
    "            ]\n",
    "            policy[state] = np.argmax(action_rewards)\n",
    "        return policy\n",
    "\n",
    "    def display_results(self):\n",
    "        print(\"Final Policy:\")\n",
    "        self.display_grid(self.policy, \"Policy\")\n",
    "        print(\"Value Function:\")\n",
    " \n",
    "    def display_grid(self, data, label):\n",
    "        grid = 3\n",
    "        symbols = [\"\\u2191\", \"\\u2193\", \"\\u2190\", \"\\u2192\"]\n",
    "        print(f\"{label}:\")\n",
    "        for i in range(grid):\n",
    "            row = []\n",
    "            for j in range(grid):\n",
    "                idx = i * grid + j\n",
    "                if data is self.policy:\n",
    "                    row.append(\"T\" if idx in [0, 2] else symbols[data[idx]])\n",
    "                else:\n",
    "                    row.append(f\"{data[idx]:.2f}\")\n",
    "            print(\" \".join(row))\n",
    "        print()\n",
    "\n",
    "reward_values = [100, 3, 0, -3]\n",
    "for reward in reward_values:\n",
    "    rewards, transitions = setup_grid_environment(reward)\n",
    "    value_opt = ValueOptimizer(rewards, transitions, gamma=0.99)\n",
    "    value_opt.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
